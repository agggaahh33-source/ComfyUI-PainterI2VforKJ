import torch
import torch.nn.functional as F
import math
from .wanvideo_utils import add_noise_to_reference_video, VAE_STRIDE, PATCH_SIZE

# ComfyUI æ ¸å¿ƒä¾èµ–
import comfy.model_management as mm
from comfy.utils import common_upscale

class PainterI2VforKJ:
    """
    å®Œå…¨ç‹¬ç«‹çš„ PainterI2V èŠ‚ç‚¹ for KJ å·¥ä½œæµ
    - ä¸ä¾èµ– ComfyUI-WanVideoWrapper æ’ä»¶
    - ç›´æŽ¥å®žçŽ°æ ¸å¿ƒç¼–ç é€»è¾‘
    - ä¿æŒä¸ŽKJå·¥ä½œæµ100%å…¼å®¹
    """
    
    @classmethod
    def INPUT_TYPES(cls):
        return {
            "required": {
                "motion_amplitude": ("FLOAT", {"default": 1.15, "min": 1.0, "max": 2.0, "step": 0.05, 
                                             "tooltip": "æ ¸å¿ƒå‚æ•°ï¼šåŠ¨æ€å¹…åº¦å¢žå¼ºç³»æ•°ï¼Œ>1.0å¢žå¼ºè¿åŠ¨å‡å°‘æ…¢åŠ¨ä½œï¼Œ1.0=ç¦ç”¨"}),
                "width": ("INT", {"default": 832, "min": 64, "max": 8096, "step": 8, "tooltip": "è¾“å‡ºè§†é¢‘å®½åº¦"}),
                "height": ("INT", {"default": 480, "min": 64, "max": 8096, "step": 8, "tooltip": "è¾“å‡ºè§†é¢‘é«˜åº¦"}),
                "num_frames": ("INT", {"default": 81, "min": 1, "max": 10000, "step": 4, "tooltip": "æ€»å¸§æ•°"}),
                "noise_aug_strength": ("FLOAT", {"default": 0.0, "min": 0.0, "max": 10.0, "step": 0.001, "tooltip": "èµ·å§‹å¸§å™ªå£°å¢žå¼ºå¼ºåº¦"}),
                "start_latent_strength": ("FLOAT", {"default": 1.0, "min": 0.0, "max": 10.0, "step": 0.001, "tooltip": "èµ·å§‹å¸§latentå¼ºåº¦"}),
                "end_latent_strength": ("FLOAT", {"default": 1.0, "min": 0.0, "max": 10.0, "step": 0.001, "tooltip": "ç»“æŸå¸§latentå¼ºåº¦"}),
                "force_offload": ("BOOLEAN", {"default": True, "tooltip": "å¤„ç†å®ŒæˆåŽå¸è½½VAEåˆ°CPU"}),
            },
            "optional": {
                "vae": ("WANVAE", {"tooltip": "WanVideo VAEæ¨¡åž‹"}),
                "clip_embeds": ("WANVIDIMAGE_CLIPEMBEDS", {"tooltip": "CLIPè§†è§‰åµŒå…¥ï¼ˆæ¥è‡ªClipVisionEncodeï¼‰"}),
                "start_image": ("IMAGE", {"tooltip": "èµ·å§‹å¸§å›¾åƒï¼ŒI2Vå¿…éœ€"}),
                "end_image": ("IMAGE", {"tooltip": "ç»“æŸå¸§å›¾åƒï¼ˆå¯é€‰ï¼‰"}),
                "control_embeds": ("WANVIDIMAGE_EMBEDS", {"tooltip": "æŽ§åˆ¶ä¿¡å·åµŒå…¥ï¼ˆFunæ¨¡åž‹ï¼‰"}),
                "fun_or_fl2v_model": ("BOOLEAN", {"default": True, "tooltip": "ä½¿ç”¨å®˜æ–¹FLF2Væˆ–Funæ¨¡åž‹æ—¶å¯ç”¨"}),
                "temporal_mask": ("MASK", {"tooltip": "æ—¶é—´æŽ©ç ï¼ŒæŽ§åˆ¶æ¯å¸§æƒé‡"}),
                "extra_latents": ("LATENT", {"tooltip": "é¢å¤–latentï¼ˆå¦‚Skyreels A2å‚è€ƒå›¾ï¼‰"}),
                "tiled_vae": ("BOOLEAN", {"default": False, "tooltip": "ä½¿ç”¨åˆ†å—VAEç¼–ç ï¼ˆçœæ˜¾å­˜ï¼‰"}),
                "add_cond_latents": ("ADD_COND_LATENTS", {"advanced": True, "tooltip": "WanVideoé¢å¤–æ¡ä»¶latent"}),
            }
        }

    RETURN_TYPES = ("WANVIDIMAGE_EMBEDS",)
    RETURN_NAMES = ("image_embeds",)
    FUNCTION = "process"
    CATEGORY = "WanVideoWrapper/PainterI2V"
    
    OUTPUT_NODE = False

    def process(self, width, height, num_frames, force_offload, noise_aug_strength,
                start_latent_strength, end_latent_strength, motion_amplitude=1.15,
                start_image=None, end_image=None, control_embeds=None, fun_or_fl2v_model=False,
                temporal_mask=None, extra_latents=None, clip_embeds=None, tiled_vae=False, add_cond_latents=None, vae=None):
        
        if start_image is None and end_image is None and add_cond_latents is None:
            return self.create_empty_embeds(num_frames, width, height, control_embeds, extra_latents)
        
        if vae is None:
            raise ValueError("âŒ VAEæ¨¡åž‹æœªæä¾›ï¼Œè¯·è¿žæŽ¥WANVAEè¾“å…¥")
        
        device = mm.get_torch_device()
        offload_device = mm.unet_offload_device()
        
        H, W = height, width
        lat_h, lat_w = H // vae.upsampling_factor, W // vae.upsampling_factor

        num_frames = ((num_frames - 1) // 4) * 4 + 1
        two_ref_images = start_image is not None and end_image is not None

        if start_image is None and end_image is not None:
            fun_or_fl2v_model = True

        base_frames = num_frames + (1 if two_ref_images and not fun_or_fl2v_model else 0)
        
        # åˆ›å»ºæ—¶é—´æŽ©ç 
        mask = self.create_temporal_mask(temporal_mask, base_frames, lat_h, lat_w, start_image, end_image, device, vae.dtype)

        # ç¼–ç å›¾åƒåºåˆ—
        vae.to(device)
        concatenated = self.prepare_image_sequence(
            vae, device, start_image, end_image, H, W, num_frames, 
            noise_aug_strength, temporal_mask, fun_or_fl2v_model
        )
        
        # æ‰§è¡ŒVAEç¼–ç  -> yå½¢çŠ¶: [C, T, H, W] (4ç»´)
        y = vae.encode([concatenated], device, end_=(end_image is not None and not fun_or_fl2v_model), tiled=tiled_vae)[0]
        del concatenated

        # å¤„ç†é¢å¤–latent
        has_ref = False
        if extra_latents is not None:
            samples = extra_latents["samples"].squeeze(0)
            y = torch.cat([samples, y], dim=1)
            mask = torch.cat([torch.ones_like(mask[:, 0:samples.shape[1]]), mask], dim=1)
            num_frames += samples.shape[1] * 4
            has_ref = True
        
        # åº”ç”¨å¼ºåº¦ç³»æ•°
        y[:, :1] *= start_latent_strength
        if y.shape[1] > 1:
            y[:, -1:] *= end_latent_strength

        # ==================== PainterI2V åŠ¨æ€å¢žå¼ºæ ¸å¿ƒç®—æ³• ====================
        if motion_amplitude > 1.0 and y.shape[1] > 1:
            print(f"\nðŸŽ¨ [PainterI2V] åº”ç”¨åŠ¨æ€å¢žå¼º: amplitude={motion_amplitude:.2f}")
            
            base_latent = y[:, 0:1]      # [C, 1, H, W]
            other_latent = y[:, 1:]      # [C, T-1, H, W]
            
            # å¹¿æ’­é¦–å¸§
            base_latent_bc = base_latent.expand(-1, other_latent.shape[1], -1, -1)
            
            # è®¡ç®—å·®å¼‚å¹¶å¢žå¼ºï¼ˆä¿æŒäº®åº¦ç¨³å®šï¼‰
            diff = other_latent - base_latent_bc
            diff_mean = diff.mean(dim=(0, 2, 3), keepdim=True)
            diff_centered = diff - diff_mean
            scaled_other = base_latent_bc + diff_centered * motion_amplitude + diff_mean
            
            # å®‰å…¨è£å‰ª
            scaled_other = torch.clamp(scaled_other, -6, 6)
            
            # é‡ç»„
            y = torch.cat([base_latent, scaled_other], dim=1)
            print("âœ… åŠ¨æ€å¢žå¼ºå®Œæˆ\n")
        # ==================== åŠ¨æ€å¢žå¼ºç»“æŸ ====================

        # è®¡ç®—åºåˆ—é•¿åº¦
        patches_per_frame = lat_h * lat_w // (PATCH_SIZE[1] * PATCH_SIZE[2])
        frames_per_stride = (num_frames - 1) // 4 + (2 if end_image is not None and not fun_or_fl2v_model else 1)
        max_seq_len = frames_per_stride * patches_per_frame

        if add_cond_latents is not None:
            add_cond_latents["ref_latent_neg"] = vae.encode(torch.zeros(1, 3, 1, H, W, device=device, dtype=vae.dtype), device)
        
        if force_offload:
            vae.model.to(offload_device)
            mm.soft_empty_cache()

        # æž„å»ºè¾“å‡º
        image_embeds = {
            "image_embeds": y,
            "clip_context": clip_embeds.get("clip_embeds", None) if clip_embeds is not None else None,
            "negative_clip_context": clip_embeds.get("negative_clip_embeds", None) if clip_embeds is not None else None,
            "max_seq_len": max_seq_len,
            "num_frames": num_frames,
            "lat_h": lat_h,
            "lat_w": lat_w,
            "control_embeds": control_embeds["control_embeds"] if control_embeds is not None else None,
            "end_image": end_image if end_image is not None else None,
            "fun_or_fl2v_model": fun_or_fl2v_model,
            "has_ref": has_ref,
            "add_cond_latents": add_cond_latents,
            "mask": mask
        }

        return (image_embeds,)
    
    def create_temporal_mask(self, temporal_mask, base_frames, lat_h, lat_w, start_image, end_image, device, dtype):
        """åˆ›å»ºå¹¶å¤„ç†æ—¶é—´æŽ©ç """
        if temporal_mask is None:
            mask = torch.zeros(1, base_frames, lat_h, lat_w, device=device, dtype=dtype)
            if start_image is not None:
                mask[:, 0:start_image.shape[0]] = 1.0
            if end_image is not None:
                mask[:, -end_image.shape[0]:] = 1.0
        else:
            mask = common_upscale(temporal_mask.unsqueeze(1).to(device), lat_w, lat_h, "nearest", "disabled").squeeze(1)
            if mask.shape[0] > base_frames:
                mask = mask[:base_frames]
            elif mask.shape[0] < base_frames:
                mask = torch.cat([mask, torch.zeros(base_frames - mask.shape[0], lat_h, lat_w, device=device)])
            mask = mask.unsqueeze(0).to(device, dtype)

        # é‡å¤æŽ©ç 
        start_mask_repeated = torch.repeat_interleave(mask[:, 0:1], repeats=4, dim=1)
        if end_image is not None:
            end_mask_repeated = torch.repeat_interleave(mask[:, -1:], repeats=4, dim=1)
            mask = torch.cat([start_mask_repeated, mask[:, 1:-1], end_mask_repeated], dim=1)
        else:
            mask = torch.cat([start_mask_repeated, mask[:, 1:]], dim=1)

        mask = mask.view(1, mask.shape[1] // 4, 4, lat_h, lat_w)
        return mask.movedim(1, 2)[0]
    
    def create_empty_embeds(self, num_frames, width, height, control_embeds=None, extra_latents=None):
        """åˆ›å»ºç©ºåµŒå…¥"""
        target_shape = (16, (num_frames - 1) // VAE_STRIDE[0] + 1,
                        height // VAE_STRIDE[1],
                        width // VAE_STRIDE[2])
        
        embeds = {
            "target_shape": target_shape,
            "num_frames": num_frames,
            "control_embeds": control_embeds["control_embeds"] if control_embeds is not None else None,
        }
        if extra_latents is not None:
            embeds["extra_latents"] = [{
                "samples": extra_latents["samples"],
                "index": 0,
            }]
        return (embeds,)
    
    def prepare_image_sequence(self, vae, device, start_image, end_image, H, W, num_frames, 
                               noise_aug_strength, temporal_mask, fun_or_fl2v_model):
        """å‡†å¤‡å›¾åƒåºåˆ—"""
        C = 3
        
        if start_image is not None:
            start_image = start_image[..., :3]
            if start_image.shape[1] != H or start_image.shape[2] != W:
                resized_start = common_upscale(start_image.movedim(-1, 1), W, H, "lanczos", "disabled").movedim(0, 1)
            else:
                resized_start = start_image.permute(3, 0, 1, 2)
            resized_start = resized_start * 2.0 - 1.0
            if noise_aug_strength > 0.0:
                resized_start = add_noise_to_reference_video(resized_start, noise_aug_strength)
            T_start = resized_start.shape[1]
        else:
            resized_start, T_start = None, 0
        
        if end_image is not None:
            end_image = end_image[..., :3]
            if end_image.shape[1] != H or end_image.shape[2] != W:
                resized_end = common_upscale(end_image.movedim(-1, 1), W, H, "lanczos", "disabled").movedim(0, 1)
            else:
                resized_end = end_image.permute(3, 0, 1, 2)
            resized_end = resized_end * 2.0 - 1.0
            if noise_aug_strength > 0.0:
                resized_end = add_noise_to_reference_video(resized_end, noise_aug_strength)
            T_end = resized_end.shape[1]
        else:
            resized_end, T_end = None, 0
        
        # æ‹¼æŽ¥
        if temporal_mask is None:
            if start_image is not None and end_image is None:
                zero_frames = torch.zeros(C, num_frames - T_start, H, W, device=device, dtype=vae.dtype)
                concatenated = torch.cat([resized_start.to(device, dtype=vae.dtype), zero_frames], dim=1)
            elif start_image is None and end_image is not None:
                zero_frames = torch.zeros(C, num_frames - T_end, H, W, device=device, dtype=vae.dtype)
                concatenated = torch.cat([zero_frames, resized_end.to(device, dtype=vae.dtype)], dim=1)
            elif start_image is None and end_image is None:
                concatenated = torch.zeros(C, num_frames, H, W, device=device, dtype=vae.dtype)
            else:
                if fun_or_fl2v_model:
                    zero_frames = torch.zeros(C, num_frames - (T_start + T_end), H, W, device=device, dtype=vae.dtype)
                else:
                    zero_frames = torch.zeros(C, num_frames - 1, H, W, device=device, dtype=vae.dtype)
                concatenated = torch.cat([resized_start.to(device, dtype=vae.dtype), zero_frames, resized_end.to(device, dtype=vae.dtype)], dim=1)
        else:
            temporal_mask = common_upscale(temporal_mask.unsqueeze(1), W, H, "nearest", "disabled").squeeze(1)
            concatenated = resized_start[:, :num_frames].to(vae.dtype)
        
        return concatenated
